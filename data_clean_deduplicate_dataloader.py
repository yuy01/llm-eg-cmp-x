import os
import shutil
import torch
import faiss
import numpy as np
import networkx as nx
from PIL import Image
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from datetime import datetime
from torch.utils.data import Dataset, DataLoader
from pathlib import Path

# --- 1. æ•°æ®é›†å®šä¹‰ (ç”¨äºå¹¶è¡ŒåŠ é€Ÿ) ---
class ImageDataset(Dataset):
    def __init__(self, image_paths):
        self.image_paths = image_paths

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        path = self.image_paths[idx]
        try:
            # convert('RGB') æå…¶é‡è¦ï¼
            # 1. è§£å†³ PNG 4é€šé“(RGBA)å¯¼è‡´æ¨¡å‹æŠ¥é”™çš„é—®é¢˜
            # 2. è§£å†³ç°åº¦å›¾ç»´åº¦ä¸åŒ¹é…é—®é¢˜
            return Image.open(path).convert('RGB')
        except Exception as e:
            # è¯»å–å¤±è´¥è¿”å›å…¨é»‘å›¾ï¼Œé¿å…ç¨‹åºå´©æºƒï¼Œåç»­å¯é€šè¿‡é€»è¾‘å‰”é™¤
            return Image.new('RGB', (224, 224), (0, 0, 0))

def custom_collate(batch):
    # SentenceTransformer éœ€è¦ List[Image]ï¼Œä¸éœ€è¦ Tensorï¼Œæ‰€ä»¥åŸæ ·è¿”å›
    return batch

# --- 2. æ ¸å¿ƒå»é‡ç±» ---
class ImageDeduplicator:
    def __init__(self, model_path, threshold=0.95):
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {os.path.basename(model_path)}...")
        if torch.backends.mps.is_available():
            self.device = 'mps'
        elif torch.cuda.is_available():
            self.device = 'cuda'
        else:
            self.device = 'cpu'
        print(f'[INFO] Device: {self.device}')
        
        self.model = SentenceTransformer(model_path, device=self.device)
        self.threshold = threshold

    def extract_features(self, image_paths):
        """æå–ç‰¹å¾ (ä½¿ç”¨ DataLoader å¹¶è¡ŒåŠ é€Ÿ)"""
        print(f"æ­£åœ¨æå– {len(image_paths)} å¼ å›¾ç‰‡çš„ç‰¹å¾...")
        
        # 1. åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨
        dataset = ImageDataset(image_paths)
        dataloader = DataLoader(
            dataset, 
            batch_size=16, 
            shuffle=False, 
            num_workers=12,      # <--- æ ¸å¿ƒåŠ é€Ÿç‚¹ï¼š8ä¸ªè¿›ç¨‹åŒæ—¶è¯»å›¾
            collate_fn=custom_collate,
            pin_memory=True     # åŠ é€Ÿ CPU -> GPU ä¼ è¾“
        )
        
        all_embeddings = []
        
        # 2. æ‰¹é‡æ¨ç†
        for batch_images in tqdm(dataloader, desc="Encoding"):
            batch_emb = self.model.encode(
                batch_images, 
                batch_size=128, 
                show_progress_bar=False, 
                convert_to_numpy=True
            )
            all_embeddings.append(batch_emb)
            
            # æ˜¾å¼å…³é—­å›¾ç‰‡å¯¹è±¡
            for img in batch_images:
                img.close()

        if not all_embeddings:
            return np.array([])
            
        embeddings = np.vstack(all_embeddings)
        embeddings = embeddings.astype('float32')
        faiss.normalize_L2(embeddings)
        
        return embeddings
    
    def find_duplicates(self, image_paths):
        """æ ¸å¿ƒé€»è¾‘ï¼šä½¿ç”¨ Range Search (èŒƒå›´æœç´¢) æ›¿ä»£ KNN"""
        embeddings = self.extract_features(image_paths)
        
        print("æ­£åœ¨æ„å»ºç´¢å¼•å¹¶è¿›è¡ŒèŒƒå›´æœç´¢...")
        index = faiss.IndexFlatIP(embeddings.shape[1])
        index.add(embeddings)
        
        # --- æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨ range_search ---
        # ä¸å†é™åˆ¶ k=50ï¼Œè€Œæ˜¯æ‰¾å‡ºæ‰€æœ‰ç›¸ä¼¼åº¦ > threshold çš„é‚»å±…
        lims, D, I = index.range_search(embeddings, self.threshold)

        print("æ­£åœ¨æ„å»ºå›¾ç»“æ„...")
        G = nx.Graph()
        G.add_nodes_from(range(len(image_paths)))
        
        # --- ä¼˜åŒ–ï¼šé¢„è¯»å–æ–‡ä»¶å¤§å° ---
        # é¿å…åœ¨åç»­æ’åºå¾ªç¯ä¸­é‡å¤è¿›è¡Œ IO æ“ä½œ
        print("é¢„è¯»å–æ–‡ä»¶å¤§å°ä»¥ä¼˜åŒ–æ’åº...")
        file_sizes = [os.path.getsize(p) for p in image_paths]

        # è§£æ range_search ç»“æœæ„å»ºå›¾
        for i in range(len(image_paths)):
            start = lims[i]
            end = lims[i+1]
            for j in range(start, end):
                neighbor_idx = I[j]
                # i < neighbor_idx ç¡®ä¿æ— å‘å›¾è¾¹åªæ·»åŠ ä¸€æ¬¡ï¼Œä¸”æ’é™¤è‡ªç¯
                if i < neighbor_idx:
                    G.add_edge(i, neighbor_idx)

        components = list(nx.connected_components(G))
        
        structured_results = []
        print(f"æ­£åœ¨åˆ†æ {len(components)} ä¸ªè¿é€šåˆ†é‡...")
        
        for component in components:
            if len(component) > 1:
                # åœ¨ç°‡å†…æŒ‰æ–‡ä»¶å¤§å°æ’åº (ä¿ç•™æœ€å¤§çš„)
                sorted_idx = sorted(list(component), key=lambda x: file_sizes[x], reverse=True)
                
                # --- ä¿ç•™ç­–ç•¥ ---
                # é»˜è®¤ï¼šåªä¿ç•™ 1 å¼ æœ€å¤§çš„
                num_to_keep = 1 
                
                # å¦‚æœä½ æƒ³æ¢å¤ä¹‹å‰çš„â€œæ¯20å¼ ç•™1å¼ â€é€»è¾‘ï¼Œå–æ¶ˆä¸‹é¢è¿™è¡Œçš„æ³¨é‡Šï¼š
                # num_to_keep = max(1, (len(component) - 1) // 20 + 1)
                
                structured_results.append({
                    'keeps': [image_paths[i] for i in sorted_idx[:num_to_keep]],
                    'duplicates': [image_paths[i] for i in sorted_idx[num_to_keep:]]
                })
        
        return structured_results

# --- 3. è¾…åŠ©åŠŸèƒ½å‡½æ•° ---
def remove_duplicates(results, mode='move', backup_dir='./duplicates_backup'):
    if not results:
        print("âœ… æ²¡æœ‰éœ€è¦åˆ é™¤çš„é‡å¤æ–‡ä»¶")
        return 0
    
    total_duplicates = sum(len(cluster['duplicates']) for cluster in results)
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š å»é‡ç»Ÿè®¡:")
    print(f"   - å‘ç°é‡å¤ç°‡: {len(results)} ä¸ª")
    print(f"   - é‡å¤æ–‡ä»¶æ€»æ•°: {total_duplicates} ä¸ª")
    print(f"   - æ“ä½œæ¨¡å¼: {'ğŸ”’ å®‰å…¨ç§»åŠ¨' if mode == 'move' else 'âš ï¸ ç›´æ¥åˆ é™¤'}")
    print(f"{'='*60}\n")
    
    confirm = input(f"âš ï¸  ç¡®è®¤è¦{('ç§»åŠ¨' if mode == 'move' else 'åˆ é™¤')} {total_duplicates} ä¸ªé‡å¤æ–‡ä»¶å—? (yes/no): ")
    if confirm.lower() not in ['yes', 'y']:
        print("âŒ æ“ä½œå·²å–æ¶ˆ")
        return 0
    
    if mode == 'move':
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_dir = f"{backup_dir}_{timestamp}"
        os.makedirs(backup_dir, exist_ok=True)
        print(f"ğŸ“ å¤‡ä»½æ–‡ä»¶å¤¹: {backup_dir}\n")
    
    removed_count = 0
    failed_files = []
    
    for i, cluster in enumerate(tqdm(results, desc="å¤„ç†ä¸­")):
        for dup_path in cluster['duplicates']:
            try:
                if mode == 'move':
                    rel_path = os.path.basename(dup_path)
                    dest_path = os.path.join(backup_dir, f"cluster_{i+1}_{rel_path}")
                    shutil.move(dup_path, dest_path)
                elif mode == 'delete':
                    os.remove(dup_path)
                removed_count += 1
            except Exception as e:
                failed_files.append((dup_path, str(e)))
    
    print(f"\nâœ… æˆåŠŸå¤„ç†: {removed_count}/{total_duplicates} ä¸ªæ–‡ä»¶")
    if failed_files:
        print(f"âš ï¸  {len(failed_files)} ä¸ªæ–‡ä»¶å¤„ç†å¤±è´¥")
    
    return removed_count

def generate_html_report(results, threshold, output_html="dedup_report.html"):
    print(f"æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š...")
    report_abs_dir = os.path.dirname(os.path.abspath(output_html))
    
    html_template = f"""
    <html>
    <head>
        <meta charset="UTF-8">
        <title>å›¾ç‰‡å»é‡æŠ¥å‘Š (Threshold: {threshold})</title>
        <style>
            body {{ font-family: sans-serif; background: #f8f9fa; padding: 20px; }}
            .cluster {{ background: white; padding: 20px; margin-bottom: 20px; border-radius: 8px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }}
            .image-grid {{ display: flex; flex-wrap: wrap; gap: 15px; }}
            .img-card {{ width: 150px; text-align: center; }}
            img {{ width: 100%; height: 150px; object-fit: cover; border-radius: 5px; border: 3px solid #eee; }}
            .keep-img {{ border-color: #28a745; }}
            .remove-img {{ border-color: #dc3545; opacity: 0.6; }}
            .badge {{ padding: 2px 6px; border-radius: 4px; color: white; font-size: 10px; }}
            .badge-keep {{ background: #28a745; }}
            .badge-remove {{ background: #dc3545; }}
        </style>
    </head>
    <body>
        <h1>å»é‡æŠ¥å‘Š (é˜ˆå€¼: {threshold})</h1>
        <p>å‘ç° {len(results)} ç»„é‡å¤ï¼Œå…± {sum(len(c['duplicates']) for c in results)} å¼ å¾…åˆ é™¤ã€‚</p>
    """

    for i, cluster in enumerate(results):
        html_template += f'<div class="cluster"><h3>Group {i+1}</h3><div class="image-grid">'
        
        for keep_path in cluster['keeps']:
            rel_path = os.path.relpath(keep_path, start=report_abs_dir)
            html_template += f"""
                <div class="img-card">
                    <span class="badge badge-keep">KEEP</span>
                    <img class="keep-img" src="{rel_path}">
                    <div style="font-size:10px">{os.path.basename(keep_path)}</div>
                </div>"""
        
        for dup_path in cluster['duplicates']:
            rel_path = os.path.relpath(dup_path, start=report_abs_dir)
            html_template += f"""
                <div class="img-card">
                    <span class="badge badge-remove">DEL</span>
                    <img class="remove-img" src="{rel_path}">
                    <div style="font-size:10px">{os.path.basename(dup_path)}</div>
                </div>"""
        html_template += '</div></div>'

    html_template += "</body></html>"
    with open(output_html, "w", encoding="utf-8") as f:
        f.write(html_template)
    print(f"âœ¨ æŠ¥å‘Šå·²ç”Ÿæˆ: {output_html}")

# --- 4. ä¸»ç¨‹åº ---
if __name__ == "__main__":
    # åˆ‡æ¢ç›®å½•
    os.chdir('/home/hadoop/data/deduplicate') 
    # ç¡®è®¤å½“å‰ç›®å½•
    print("å½“å‰å·¥ä½œç›®å½•æ˜¯:", os.getcwd())
    
    # é…ç½®
    # MODEL_PATH = "/home/hadoop/.cache/modelscope/hub/models/sentence-transformers/clip-ViT-B-32"
    MODEL_PATH = "/home/hadoop/data/model_download/sentence-transformers/clip-ViT-B-32"
    # IMAGE_DIR = '/home/hadoop/data/feedcld/data/ç™½æ ·æœ¬'
    # IMAGE_DIR = '/home/hadoop/data/ç£åŠ›çƒ/data/ç£åŠ›çƒ-è´¨æ£€å'
    # IMAGE_DIR = '/home/hadoop/data/cldfeed/data/ç™½æ ·æœ¬'
    # IMAGE_DIR = '/home/hadoop/data/cldfeed/realdata/realdata_67w'
    # IMAGE_DIR = '/home/hadoop/data/cldfeed/IC_datas/IC_4class_round2_deduplicate/Ratio_30_1'
    IMAGE_DIR = '/home/hadoop/data/share/Tongying_dataset/Black/child_cloth_fire'
    OUTPUT_DIR = '/home/hadoop/data/deduplicate'
    
    THRESHOLD = 0.98

    # æ‰«æ
    EXTS = ('.jpg', '.jpeg', '.png', '.webp', '.bmp', '.gif')
    all_images = []
    print(f"æ­£åœ¨æ‰«æ: {IMAGE_DIR}")
    for root, dirs, files in os.walk(IMAGE_DIR):
        for file in files:
            if file.lower().endswith(EXTS):
                all_images.append(os.path.join(root, file))

    print(f"æ‰¾åˆ° {len(all_images)} å¼ å›¾ç‰‡")

    if all_images:

        
        
        deduper = ImageDeduplicator(MODEL_PATH, threshold=THRESHOLD)
        results = deduper.find_duplicates(all_images)
        
        if results:
            name = os.path.basename(IMAGE_DIR)
            current_dir = Path(__file__).parent.resolve()

            generate_html_report(results, THRESHOLD, output_html=f'{OUTPUT_DIR}/{name}_{THRESHOLD}.html')
            
            print("\nè¯·é€‰æ‹©æ“ä½œæ¨¡å¼:")
            print("1. ç§»åŠ¨é‡å¤æ–‡ä»¶åˆ°å¤‡ä»½ (æ¨è)")
            print("2. ç›´æ¥åˆ é™¤")
            print("3. é€€å‡º")
            choice = input("è¾“å…¥é€‰é¡¹: ").strip()
            
            if choice == '1':
                remove_duplicates(results, mode='move')
            elif choice == '2':
                remove_duplicates(results, mode='delete')
            elif choice == '3':
                print("âœ… å·²è·³è¿‡åˆ é™¤æ“ä½œ")
            else:
                print("âŒ æ— æ•ˆé€‰é¡¹ï¼Œå·²å–æ¶ˆæ“ä½œ")
            # deduper1 = ImageDeduplicator(MODEL_PATH, threshold=0.96)
            # results1 = deduper.find_duplicates(all_images)
            # emove_duplicates(results1, mode='delete')
        else:
            print("âœ… æœªå‘ç°é‡å¤å›¾ç‰‡")
