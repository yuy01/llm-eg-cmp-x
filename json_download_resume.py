import os
import json
import csv
import requests
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed

# ================= é…ç½®åŒºåŸŸ =================
# åŸæœ‰çš„æ–‡ä»¶å¤¹é…ç½®
JSON_DIR = "/home/hadoop/data/cldfeed/data_clean/data_cldfeed/cld_bottle_black_audit_datas"

# SAVE_DIR = "/home/hadoop/data/cldfeed/data_clean/data_cldfeed/cld_bottle_black_audit_datas/cld_bottle_black_audit_datas_download"
# SAVE_DIR = "/home/hadoop/data/cldfeed/data_clean/data_cldfeed/json_download_20260122"
# SAVE_DIR = "/home/hadoop/data/cldfeed/data_clean/data_cldfeed/json_download/other_data"
SAVE_DIR = "/home/hadoop/data/cldfeed/dinov3/inference_results/infer_01281338/black_pictures"
SAVE_DIR = "/home/hadoop/data/cldfeed/data_clean/data_cldfeed/json_download/sample_datas_1_0128_infer"
SAVE_DIR = "/Users/10294814/task/cldfeed/abner/data_clean/data_cldfeed/audit_result_0202"

# ã€æ–°å¢åŠŸèƒ½ã€‘æŒ‡å®šå•ä¸ªJSONæ–‡ä»¶è·¯å¾„
# å¦‚æœè¿™ä¸ªå˜é‡ä¸ä¸º None ä¸”è·¯å¾„å­˜åœ¨ï¼Œè„šæœ¬å°†å¿½ç•¥ JSON_DIRï¼Œåªå¤„ç†è¿™ä¸ªæ–‡ä»¶
# ç¤ºä¾‹ï¼šSINGLE_JSON_PATH = "/home/hadoop/data/test_data.json"
SINGLE_JSON_PATH = None 
# SINGLE_JSON_PATH = "/home/hadoop/data/cldfeed/data_clean/data_cldfeed/20260122.json" # è§£é™¤æ³¨é‡Šä»¥å¯ç”¨å•æ–‡ä»¶æ¨¡å¼
# SINGLE_JSON_PATH = "/home/hadoop/data/cldfeed/data_clean/data_cldfeed/other_data.json" # è§£é™¤æ³¨é‡Šä»¥å¯ç”¨å•æ–‡ä»¶æ¨¡å¼
SINGLE_JSON_PATH = "/home/hadoop/data/cldfeed/data_clean/data_cldfeed/20260126.json" # è§£é™¤æ³¨é‡Šä»¥å¯ç”¨å•æ–‡ä»¶æ¨¡å¼
SINGLE_JSON_PATH = "/home/hadoop/data/cldfeed/data_clean/data_cldfeed/20260122.json" # è§£é™¤æ³¨é‡Šä»¥å¯ç”¨å•æ–‡ä»¶æ¨¡å¼
SINGLE_JSON_PATH = "/home/hadoop/data/cldfeed/data_clean/data_cldfeed/sample_datas_1.json" # è§£é™¤æ³¨é‡Šä»¥å¯ç”¨å•æ–‡ä»¶æ¨¡å¼
SINGLE_JSON_PATH = "/Users/10294814/task/cldfeed/abner/data_clean/data_cldfeed/audit_result_0202.json" # è§£é™¤æ³¨é‡Šä»¥å¯ç”¨å•æ–‡ä»¶æ¨¡å¼

# ã€æ–°å¢åŠŸèƒ½ã€‘æŒ‡å®šCSVæ–‡ä»¶è·¯å¾„ï¼ˆåŸºäºæ¨ç†ç»“æœä¸‹è½½ï¼‰
# å¦‚æœè¿™ä¸ªå˜é‡ä¸ä¸º None ä¸”è·¯å¾„å­˜åœ¨ï¼Œè„šæœ¬å°†ï¼š
# 1. è¯»å–CSVä¸­åŒ…å«å›¾ç‰‡æ–‡ä»¶åçš„åˆ— (æ ¼å¼å¦‚ sn_xxxx.jpg æˆ– sn.jpg)
# 2. æå–snå·
# 3. å»JSONä¸­æŸ¥æ‰¾å¯¹åº”çš„urlå¹¶ä¸‹è½½
# æ³¨æ„ï¼šå¼€å¯æ­¤æ¨¡å¼æ—¶ï¼Œä¼šå¿½ç•¥ä¸‹æ–¹çš„ WHITE_TOP_SCORE ç­›é€‰é€»è¾‘ï¼Œåªä¸‹è½½CSVä¸­å­˜åœ¨çš„sn
CSV_PATH = None
# CSV_PATH = "/home/hadoop/data/cldfeed/dinov3/inference_results/infer_01281338/8_model_20260128_142628_predictions.csv" # è§£é™¤æ³¨é‡Šä»¥å¯ç”¨CSVç­›é€‰æ¨¡å¼

# ã€æ–°å¢é…ç½®ã€‘æŒ‡å®šéœ€è¦ä¸‹è½½çš„é¢„æµ‹ç±»åˆ«
# åœ¨è¿™é‡Œå®šä¹‰éœ€è¦ç­›é€‰çš„ predicted_classï¼Œå¯ä»¥æœ‰å¤šä¸ª
# å¦‚æœç•™ç©º []ï¼Œåœ¨CSVæ¨¡å¼ä¸‹è¡¨ç¤ºä¸ç­›é€‰ç±»åˆ«ï¼Œå…¨éƒ¨åŒ¹é…
TARGET_PREDICTED_CLASSES = [] 

# TARGET_PREDICTED_CLASSES = ["cld_bottle_pillow"] 
# ä»¥åå¦‚æœéœ€è¦å¢åŠ å…¶ä»–ç±»ï¼Œå¯ä»¥è¿™æ ·å†™ï¼š TARGET_PREDICTED_CLASSES = ["cld_bottle_pillow", "other_class"]


MAX_WORKERS = 20

# whiteæ ‡ç­¾çš„top_scoreç­›é€‰åŒºé—´
WHITE_TOP_SCORE_MIN = 0.0  # æœ€å°é˜ˆå€¼
WHITE_TOP_SCORE_MAX = 1.0  # æœ€å¤§é˜ˆå€¼

# ã€æ–°å¢é…ç½®ã€‘
# 1. æ˜¯å¦è·³è¿‡ä¸»ç¨‹åºçš„æœ¬åœ°æ–‡ä»¶é¢„æ£€æŸ¥
# True: ä¸åœ¨ä¸»çº¿ç¨‹éå†æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆå¯åŠ¨æå¿«ï¼‰ï¼Œç›´æ¥å…¨éƒ¨è¿›å…¥ä¸‹è½½é˜Ÿåˆ—ï¼Œç”±ä¸‹è½½çº¿ç¨‹å†³å®šæ˜¯å¦ä¸‹è½½ã€‚
# False: å…ˆéå†æ£€æŸ¥æœ¬åœ°æ–‡ä»¶ï¼ˆå¯åŠ¨è¾ƒæ…¢ï¼‰ï¼Œåªå°†ä¸å­˜åœ¨çš„æ–‡ä»¶æ”¾å…¥ä¸‹è½½é˜Ÿåˆ—ã€‚
SKIP_PRE_CHECK = True  

# 2. æ˜¯å¦å¼ºåˆ¶è¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶
# True: å³ä½¿æ–‡ä»¶å­˜åœ¨ä¹Ÿé‡æ–°ä¸‹è½½ï¼ˆå¸¸ç”¨äºä¿®å¤æŸåæ–‡ä»¶æˆ–æ›´æ–°å›¾ç‰‡ï¼‰ã€‚
# False: å¦‚æœæ–‡ä»¶å­˜åœ¨åˆ™è·³è¿‡ï¼ˆèŠ‚çœå¸¦å®½å’Œæ—¶é—´ï¼‰ã€‚
FORCE_OVERWRITE = False

# ã€æ–°å¢é…ç½®ã€‘æŒ‡å®šJSONä¸­å›¾ç‰‡é“¾æ¥çš„å­—æ®µå
# ä½ çš„æ–°æ•°æ®æ˜¯ "img_url"ï¼Œæ—§æ•°æ®å¯èƒ½æ˜¯ "img_orgn_url"
# è¯·åœ¨æ­¤å¤„å®šä¹‰ï¼Œåç»­ä»£ç å°†ä½¿ç”¨è¿™ä¸ªå˜é‡
IMG_URL_KEY = "img_url" 
# IMG_URL_KEY = "img_orgn_url" 

# ã€æ–°å¢é…ç½®ã€‘ æ˜¯å¦ä¸‹è½½æ‰€æœ‰æ•°æ®ï¼ˆå¿½ç•¥infer_labelå’Œåˆ†æ•°ç­›é€‰ï¼‰
# True: ä¸ç®¡æ ‡ç­¾æ˜¯whiteè¿˜æ˜¯å…¶ä»–ï¼Œä¹Ÿä¸ç®¡åˆ†æ•°å¤šå°‘ï¼ŒJSONé‡Œæœ‰ä»€ä¹ˆå°±ä¸‹è½½ä»€ä¹ˆ
# False: æŒ‰ç…§ infer_label å’Œ WHITE_TOP_SCORE é€»è¾‘ç­›é€‰
DOWNLOAD_ALL_DATA = True 

# ã€æ–°å¢é…ç½®ã€‘ IDX èŒƒå›´ç­›é€‰ (idxé€šå¸¸ä¸ºæ•´æ•°)
# å¦‚æœä¸æƒ³æŒ‰idxç­›é€‰ï¼Œè¯·å°†ä»¥ä¸‹ä¸¤ä¸ªå˜é‡è®¾ç½®ä¸º None
# ç¤ºä¾‹ï¼šåªä¸‹è½½ idx ä¸º 1 åˆ° 50 çš„æ•°æ® -> IDX_MIN = 1, IDX_MAX = 50
IDX_MIN = 1       # èµ·å§‹ idx (åŒ…å«)
IDX_MAX = 10      # ç»“æŸ idx (åŒ…å«)
# è‹¥è¦å…³é—­ idx ç­›é€‰ï¼Œè¯·è§£å¼€ä¸‹é¢ä¸¤è¡Œæ³¨é‡Šï¼š
# IDX_MIN = None
# IDX_MAX = None

# ã€æ–°å¢é…ç½®ã€‘ æ˜¯å¦åœ¨æ–‡ä»¶åå‰æ·»åŠ  idx (ä¾‹å¦‚: 1_sn123_img.jpg)
# True: æ·»åŠ  idx_ å‰ç¼€ (å‰ææ˜¯æ•°æ®ä¸­æœ‰idxå­—æ®µ)
# False: ä¸æ·»åŠ 
ADD_IDX_TO_FILENAME = True
ADD_IDX_TO_FILENAME = False
# ===========================================

os.makedirs(SAVE_DIR, exist_ok=True)

# def get_save_path(item, save_dir):
#     """
#     æ ¹æ®itemä¿¡æ¯ç”Ÿæˆæœ¬åœ°ä¿å­˜çš„ç»å¯¹è·¯å¾„ã€‚
#     """
#     img_url = item.get(IMG_URL_KEY)
#     if not img_url:
#         return None
    
#     filename = os.path.basename(img_url)
#     if not filename:
#         filename = f"{item.get('goods_sn', 'unknown')}.jpg"
    
#     return os.path.join(save_dir, filename)

def get_save_path(item, save_dir):
    """
    æ ¹æ®itemä¿¡æ¯ç”Ÿæˆæœ¬åœ°ä¿å­˜çš„ç»å¯¹è·¯å¾„ã€‚
    ã€ä¿®æ”¹ã€‘å¼ºåˆ¶ä½¿ç”¨ goods_sn å‘½åå›¾ç‰‡
    ã€ä¿®æ”¹ã€‘æ”¯æŒåœ¨æ–‡ä»¶åå‰æ·»åŠ  idx
    ã€ä¿®æ”¹ã€‘æ”¯æŒæ ¹æ® label å»ºç«‹å­æ–‡ä»¶å¤¹
    """
    # ã€ä¿®æ”¹ã€‘ä½¿ç”¨é…ç½®çš„ IMG_URL_KEY è·å–é“¾æ¥
    img_url = item.get(IMG_URL_KEY)
    goods_sn = item.get('goods_sn')

    # å¦‚æœæ²¡æœ‰å›¾ç‰‡é“¾æ¥æˆ–æ²¡æœ‰snå·ï¼Œåˆ™æ— æ³•æ­£ç¡®å‘½åï¼Œè·³è¿‡
    if not img_url or not goods_sn:
        return None
    # æ„é€ æ–‡ä»¶å
    
    # 1. è·å–URLæœ€åçš„æ–‡ä»¶åéƒ¨åˆ† (e.g., img_01.png æˆ– img_02.webp?v=1)
    url_base_name = os.path.basename(img_url)
    
    # 2. å»é™¤å¯èƒ½å­˜åœ¨çš„URLå‚æ•°ï¼ˆå³ ? ä¹‹åçš„å†…å®¹ï¼‰
    url_base_name_clean = url_base_name.split('?')[0]
    
    # 3. ä½¿ç”¨ os.path.splitext åˆ†ç¦»æ–‡ä»¶åå’ŒåŸæœ‰åç¼€ 
    # (e.g. 'my_image_01.png' -> 'my_image_01')
    # æ³¨æ„ï¼šè¿™é‡Œçš„ name_without_ext ä¼šå®Œæ•´ä¿ç•™åŸæ–‡ä»¶åä¸­çš„ä¸‹åˆ’çº¿
    name_without_ext = os.path.splitext(url_base_name_clean)[0]
    
    # 4. å¼ºåˆ¶æ‹¼æ¥ .jpg åç¼€
    # åŸºç¡€æ–‡ä»¶å
    filename = f"{goods_sn}_{name_without_ext}.jpg"
    
    # ã€æ–°å¢é€»è¾‘ã€‘å¦‚æœå¼€å¯äº†æ·»åŠ IDXä¸”æ•°æ®ä¸­æœ‰idxï¼Œåˆ™æ‹¼æ¥åˆ°æœ€å‰é¢
    if ADD_IDX_TO_FILENAME:
        idx = item.get('idx')
        if idx is not None:
            filename = f"{idx}_{filename}"
    
    # ã€æ–°å¢é€»è¾‘ã€‘å¤„ç† label æ–‡ä»¶å¤¹
    # è·å– label å­—æ®µ
    label = item.get('label')
    if label:
        # æ¸…æ´— label åç§°ï¼Œé˜²æ­¢åŒ…å«éæ³•å­—ç¬¦ï¼ˆå¦‚ / æˆ– \ï¼‰ï¼Œå°†å…¶æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
        safe_label_name = str(label).replace('/', '_').replace('\\', '_').strip()
        # å°†ä¿å­˜è·¯å¾„æŒ‡å‘å­æ–‡ä»¶å¤¹
        save_dir = os.path.join(save_dir, safe_label_name)

    return os.path.join(save_dir, filename)

def get_sns_from_csv(csv_path, target_classes):
    """
    ä»CSVæ–‡ä»¶ä¸­è¯»å–æ•°æ®ï¼Œç­›é€‰æŒ‡å®šç±»åˆ«çš„è¡Œï¼Œå¹¶æå–SNå·ã€‚
    ä½¿ç”¨ csv.DictReader åŸºäºè¡¨å¤´è¯»å– (image_name, predicted_class, score)
    """
    target_sns = set()
    print(f"æ­£åœ¨ä»CSVè¯»å–SN: {csv_path}")
    print(f"ç›®æ ‡ç­›é€‰ç±»åˆ«: {target_classes}")

    try:
        with open(csv_path, 'r', encoding='utf-8', errors='ignore') as f:
            # ä½¿ç”¨ DictReader è‡ªåŠ¨è¯†åˆ«è¡¨å¤´
            reader = csv.DictReader(f)
            
            for row in reader:
                # 1. è·å–å½“å‰è¡Œçš„é¢„æµ‹ç±»åˆ« (å»é™¤å‰åç©ºæ ¼)
                p_class = row.get('predicted_class', '').strip()
                
                # 2. åˆ¤æ–­æ˜¯å¦åœ¨ç›®æ ‡ç±»åˆ«ä¸­
                # ã€ä¿®æ”¹ã€‘å¦‚æœ target_classes ä¸ä¸ºç©ºï¼Œæ‰è¿›è¡Œç­›é€‰ï¼›ä¸ºç©ºåˆ™é»˜è®¤å…¨éƒ¨åŒ…å«
                if target_classes and p_class not in target_classes:
                    continue
                
                # 3. è·å–æ–‡ä»¶å
                filename = row.get('image_name', '').strip()
                if not filename:
                    continue
                
                # 4. æå–SNé€»è¾‘
                # æ”¯æŒ sn_xxxx.jpg å’Œ sn.jpg
                # å»é™¤åç¼€ (pants181025908.jpg -> pants181025908)
                name_no_ext = os.path.splitext(filename)[0]
                
                # æå–SNï¼šå–ç¬¬ä¸€ä¸ªä¸‹åˆ’çº¿å‰é¢çš„éƒ¨åˆ†
                # å¦‚æœæ²¡æœ‰ä¸‹åˆ’çº¿ï¼Œsplitè¿”å›åŸå­—ç¬¦ä¸²ï¼Œé€»è¾‘é€šç”¨
                sn = name_no_ext.split('_')[0]
                
                if sn:
                    target_sns.add(sn)

    except Exception as e:
        print(f"è¯»å–CSVå¤±è´¥: {e}")
        print("è¯·ç¡®ä¿CSVæ–‡ä»¶åŒ…å«è¡¨å¤´: image_name, predicted_class")
    
    print(f"ä»CSVä¸­æå–åˆ° {len(target_sns)} ä¸ªç¬¦åˆæ¡ä»¶çš„å”¯ä¸€SNå·")
    return target_sns

def download_one(item):
    """
    ä¸‹è½½å•å¼ å›¾ç‰‡
    """
    # ã€ä¿®æ”¹ã€‘ä½¿ç”¨é…ç½®çš„ IMG_URL_KEY è·å–é“¾æ¥
    img_url = item.get(IMG_URL_KEY)
    if not img_url:
        return

    try:
        # è·å–ä¿å­˜è·¯å¾„
        save_path = get_save_path(item, SAVE_DIR)
        if not save_path:
            return
        
        # ã€æ–°å¢é€»è¾‘ã€‘ç¡®ä¿å­æ–‡ä»¶å¤¹å­˜åœ¨ï¼ˆå› ä¸ºè·¯å¾„ç°åœ¨åŒ…å«äº†labelå­ç›®å½•ï¼‰
        save_folder = os.path.dirname(save_path)
        if not os.path.exists(save_folder):
            os.makedirs(save_folder, exist_ok=True)
        
        # ã€ä¿®æ”¹é€»è¾‘ã€‘å¦‚æœä¸å¼ºåˆ¶è¦†ç›–ï¼Œä¸”æ–‡ä»¶å­˜åœ¨ä¸”å¤§å°å¤§äº0ï¼Œåˆ™è·³è¿‡
        if not FORCE_OVERWRITE:
            # åŒé‡æ£€æŸ¥ï¼šé˜²æ­¢å¹¶å‘å†™å…¥
            if os.path.exists(save_path) and os.path.getsize(save_path) > 0:
                return

        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(img_url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            # å†™å…¥ä¸´æ—¶æ–‡ä»¶å†é‡å‘½åï¼Œé˜²æ­¢ä¸­æ–­å¯¼è‡´ç ´æŸæ–‡ä»¶
            temp_path = save_path + ".tmp"
            with open(temp_path, 'wb') as f:
                f.write(response.content)
            os.rename(temp_path, save_path)
        else:
            pass
    except Exception as e:
        pass

def collect_items(source_path, target_sns=None):
    """
    ã€ä¿®æ”¹ã€‘æ”¶é›†ç¬¦åˆæ¡ä»¶çš„æ¡ç›®
    source_path: å¯ä»¥æ˜¯æ–‡ä»¶å¤¹è·¯å¾„ï¼Œä¹Ÿå¯ä»¥æ˜¯å•ä¸ªjsonæ–‡ä»¶è·¯å¾„
    target_sns: seté›†åˆï¼Œå¦‚æœä¼ å…¥ä¸ä¸ºNoneï¼Œåˆ™åªç­›é€‰goods_snåœ¨é›†åˆä¸­çš„item
    """
    items = []
    if not os.path.exists(source_path):
        print(f"é”™è¯¯: è·¯å¾„ä¸å­˜åœ¨ -> {source_path}")
        return items

    # ã€æ ¸å¿ƒé€»è¾‘ä¿®æ”¹ã€‘åˆ¤æ–­ä¼ å…¥çš„æ˜¯æ–‡ä»¶è¿˜æ˜¯ç›®å½•ï¼Œç”Ÿæˆå¾…å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
    file_list = []
    if os.path.isfile(source_path):
        print(f"æ­£åœ¨è¯»å– å•ä¸ªJSON æ–‡ä»¶: {source_path}")
        file_list.append(source_path)
    else:
        # å¦‚æœæ˜¯ç›®å½•ï¼Œéå†ç›®å½•ä¸‹çš„json
        raw_files = [f for f in os.listdir(source_path) if f.lower().endswith(".json")]
        print(f"æ­£åœ¨è¯»å– ç›®å½•ä¸‹çš„ JSON æ–‡ä»¶ ({len(raw_files)} ä¸ª)...")
        file_list = [os.path.join(source_path, f) for f in raw_files]

    # ç»Ÿä¸€å¤„ç†æ–‡ä»¶åˆ—è¡¨
    for file_path in file_list:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
                # ç¡®ä¿dataæ˜¯åˆ—è¡¨
                if not isinstance(data, list):
                    continue

                filtered = []
                for item in  data:# åŠ ä¸Šäº† data å’Œå†’å·
                    # ã€ä¿®æ”¹ã€‘è¿™é‡Œä½¿ç”¨ IMG_URL_KEY æ¥åˆ¤æ–­æ˜¯å¦æœ‰å›¾ç‰‡é“¾æ¥
                    if not isinstance(item, dict) or not item.get(IMG_URL_KEY):
                        continue

                    # ==================== IDX ç­›é€‰é€»è¾‘ ====================
                    # è·å–å½“å‰itemçš„idx
                    item_idx = item.get('idx')
                    
                    # ä»…å½“ ç”¨æˆ·è®¾ç½®äº†èŒƒå›´ ä¸” å½“å‰æ•°æ®å­˜åœ¨idx æ—¶ï¼Œæ‰è¿›è¡Œåˆ¤æ–­
                    if IDX_MIN is not None and IDX_MAX is not None and item_idx is not None:
                        try:
                            val = int(item_idx)
                            # å¦‚æœä¸åœ¨èŒƒå›´å†…ï¼Œåˆ™è·³è¿‡
                            if not (IDX_MIN <= val <= IDX_MAX):
                                continue
                        except ValueError:
                            # å¦‚æœidxè½¬ä¸æˆæ•°å­—ï¼Œé»˜è®¤å¿½ç•¥è¯¥æ¡ä»¶ï¼Œç»§ç»­ä¸‹è½½ï¼ˆé˜²æ­¢æ¼ä¸‹ï¼‰
                            pass
                    # ====================================================
                    
                    goods_sn = str(item.get('goods_sn', ''))

                    # logic branch 1: å¦‚æœæœ‰ target_sns (CSVæ¨¡å¼)ï¼Œåˆ™ä¼˜å…ˆåŒ¹é… SN
                    if target_sns is not None:
                        # æ£€æŸ¥å½“å‰ item çš„ sn æ˜¯å¦åœ¨ CSV æå–çš„åå•é‡Œ
                        if goods_sn in target_sns:
                            filtered.append(item)
                        continue # CSVæ¨¡å¼ä¸‹ï¼Œåªè¦SNå¯¹ä¸Šå°±åŠ å…¥ï¼Œä¸çœ‹labelï¼Œç›´æ¥è¿›å…¥ä¸‹ä¸€ä¸ªå¾ªç¯

                    # logic branch 2: åŸæœ‰é€»è¾‘ (Whiteæ ‡ç­¾ç­›é€‰)
                    # ã€ä¿®æ”¹ã€‘å¦‚æœå¼€å¯äº† DOWNLOAD_ALL_DATAï¼Œåˆ™ç›´æ¥åŠ å…¥ä¸‹è½½é˜Ÿåˆ—ï¼Œå¿½ç•¥æ ‡ç­¾åˆ¤æ–­
                    if DOWNLOAD_ALL_DATA:
                        filtered.append(item)
                        continue

                    infer_label = item.get('infer_label')
                    if infer_label != 'white':
                        # éwhiteæ ‡ç­¾çš„å›¾ç‰‡ï¼Œç›´æ¥ä¸‹è½½
                        filtered.append(item)
                    else:
                        # whiteæ ‡ç­¾çš„å›¾ç‰‡ï¼Œæ ¹æ®top_scoreåŒºé—´ç­›é€‰
                        top_score = item.get('top_score', 0)
                        if WHITE_TOP_SCORE_MIN <= top_score <= WHITE_TOP_SCORE_MAX:
                            filtered.append(item)
                
                items.extend(filtered)
        except Exception as e:
            print(f"è¯»å–{file_path}å¤±è´¥: {e}")
            
    return items

def main():
    print("="*50)
    
    # ã€æ–°å¢ã€‘é€»è¾‘åˆ¤æ–­ï¼šå†³å®šå¤„ç†ç›®æ ‡æ˜¯ æ–‡ä»¶å¤¹ è¿˜æ˜¯ å•ä¸ªæ–‡ä»¶
    target_source = JSON_DIR
    mode_msg = "æ–‡ä»¶å¤¹æ¨¡å¼"
    
    if SINGLE_JSON_PATH and os.path.exists(SINGLE_JSON_PATH):
        target_source = SINGLE_JSON_PATH
        mode_msg = "å•æ–‡ä»¶æ¨¡å¼"
    elif SINGLE_JSON_PATH and not os.path.exists(SINGLE_JSON_PATH):
        print(f"è­¦å‘Š: æŒ‡å®šçš„å•æ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨: {SINGLE_JSON_PATH}ï¼Œå°†å›é€€åˆ°æ–‡ä»¶å¤¹æ¨¡å¼ã€‚")

    # ã€æ–°å¢ã€‘CSV æ¨¡å¼åˆ¤æ–­
    target_sns_set = None
    if CSV_PATH and os.path.exists(CSV_PATH):
        print(f"CSVæ¨¡å¼å·²å¼€å¯ï¼Œå°†æ ¹æ®CSVæ–‡ä»¶ç­›é€‰SN: {CSV_PATH}")
        # ä¼ å…¥éœ€è¦ç­›é€‰çš„ç±»åˆ«åˆ—è¡¨
        target_sns_set = get_sns_from_csv(CSV_PATH, TARGET_PREDICTED_CLASSES)
    elif CSV_PATH:
         print(f"è­¦å‘Š: æŒ‡å®šçš„CSVè·¯å¾„ä¸å­˜åœ¨: {CSV_PATH}ï¼Œå°†å›é€€åˆ°æ™®é€šç­›é€‰æ¨¡å¼ã€‚")

    print(f"è¿è¡Œæ¨¡å¼: {mode_msg}")
    print(f"å¤„ç†è·¯å¾„: {target_source}")
    print(f"ä¿å­˜æ–‡ä»¶å¤¹: {SAVE_DIR}")
    if target_sns_set is None:
        if DOWNLOAD_ALL_DATA:
            print("ç­›é€‰æ¨¡å¼: [å…¨é‡ä¸‹è½½] å¿½ç•¥æ ‡ç­¾å’Œåˆ†æ•°ï¼Œä¸‹è½½æ‰€æœ‰å†…å®¹")
        else:
            print(f"ç­›é€‰æ¨¡å¼: Whiteæ ‡ç­¾ç­›é€‰åŒºé—´: {WHITE_TOP_SCORE_MIN} - {WHITE_TOP_SCORE_MAX}")
    else:
        print(f"ç­›é€‰æ¨¡å¼: ä»…ä¸‹è½½ CSV ä¸­ç±»åˆ«ä¸º {TARGET_PREDICTED_CLASSES if TARGET_PREDICTED_CLASSES else 'ALL'} çš„ {len(target_sns_set)} ä¸ª SN")
    
    # æ‰“å° IDX ç­›é€‰çŠ¶æ€
    if IDX_MIN is not None and IDX_MAX is not None:
        print(f"IDX èŒƒå›´: {IDX_MIN} - {IDX_MAX} (åŒ…å«)")
    else:
        print("IDX èŒƒå›´: ä¸ç­›é€‰ (å…¨é‡æˆ–ä¸å­˜åœ¨ idx)")

    print(f"è·³è¿‡æœ¬åœ°é¢„æ£€: {SKIP_PRE_CHECK}")
    print(f"å¼ºåˆ¶è¦†ç›–ä¸‹è½½: {FORCE_OVERWRITE}")
    print(f"å›¾ç‰‡å­—æ®µKey : {IMG_URL_KEY}")
    print(f"æ–‡ä»¶ååŠ IDX : {ADD_IDX_TO_FILENAME}")
    print("="*50)

    # 1. æ”¶é›†æ‰€æœ‰ç¬¦åˆä¸šåŠ¡é€»è¾‘çš„æ¡ç›® (ä¼ å…¥ target_source å’Œ å¯èƒ½å­˜åœ¨çš„ target_sns_set)
    target_items = collect_items(target_source, target_sns=target_sns_set)
    
    # ç»Ÿè®¡å„æ ‡ç­¾çš„æ•°é‡
    label_stats = {}
    for item in target_items:
        # ä¼˜å…ˆå– infer_labelï¼Œå–ä¸åˆ°å– labelï¼Œéƒ½å–ä¸åˆ°åˆ™ unknown
        label = item.get('infer_label') or item.get('label') or 'unknown'
        label_stats[label] = label_stats.get(label, 0) + 1
    
    print("\n[æ ‡ç­¾ç»Ÿè®¡]:")
    for label, count in label_stats.items():
        print(f"  {label}: {count}")


    need_download_items = [] # åˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨
    existing_count = 0

    if SKIP_PRE_CHECK:
        print("\næç¤º: å·²é…ç½®è·³è¿‡æœ¬åœ°æ–‡ä»¶å¯¹æ¯”ï¼Œæ‰€æœ‰å›¾ç‰‡å‡åŠ å…¥ä¸‹è½½é˜Ÿåˆ—...")
        need_download_items = target_items
        existing_count = 0 # æ— æ³•ç»Ÿè®¡ï¼Œç½®ä¸º0
    else:
        print("\næ­£åœ¨æ¯”å¯¹æœ¬åœ°æ–‡ä»¶...")
        for item in tqdm(target_items, desc="æ¯”å¯¹æœ¬åœ°æ–‡ä»¶"):
            # å¦‚æœé…ç½®äº†å¼ºåˆ¶è¦†ç›–ï¼Œåˆ™ä¸éœ€è¦æ£€æŸ¥æœ¬åœ°æ˜¯å¦å­˜åœ¨ï¼Œç›´æ¥åŠ å…¥ä¸‹è½½åˆ—è¡¨
            if FORCE_OVERWRITE:
                need_download_items.append(item)
                continue

            save_path = get_save_path(item, SAVE_DIR)
            if save_path and os.path.exists(save_path) and os.path.getsize(save_path) > 0:
                existing_count += 1
            else:
                need_download_items.append(item)

    # 3. æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("="*50)
    print(f"JSONç»Ÿè®¡ç¬¦åˆæ¡ä»¶æ€»æ•°: {len(target_items)}")
    if not SKIP_PRE_CHECK and not FORCE_OVERWRITE:
        print(f"æ–‡ä»¶å¤¹å·²å­˜åœ¨å›¾ç‰‡æ•°é‡: {existing_count}")
    print(f"æœ¬æ¬¡éœ€ä¸‹è½½å›¾ç‰‡æ•°é‡  : {len(need_download_items)}")
    print("="*50)

    if not need_download_items:
        print("âœ… æ‰€æœ‰å›¾ç‰‡å‡å·²å­˜åœ¨ï¼Œæ— éœ€ä¸‹è½½ã€‚")
        return

    # 4. æ‰§è¡Œä¸‹è½½
    print(f"\nğŸš€ å¯åŠ¨ {MAX_WORKERS} ä¸ªçº¿ç¨‹å¼€å§‹ä¸‹è½½...")
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(download_one, item) for item in need_download_items]
        for _ in tqdm(as_completed(futures), total=len(need_download_items), desc="ä¸‹è½½è¿›åº¦"):
            pass

    print(f"\nâœ… ä»»åŠ¡å…¨éƒ¨å®Œæˆï¼Œå›¾ç‰‡ä¿å­˜åœ¨: {SAVE_DIR}")

if __name__ == '__main__':
    main()
